# è¯„ä¼°å™¨Promptç®¡ç†ä¼˜åŒ– - å®Œæˆæ€»ç»“

## âœ… ä¼˜åŒ–ç›®æ ‡

å°†ç¡¬ç¼–ç åœ¨ä»£ç ä¸­çš„è¯„ä¼°Promptè¿ç§»åˆ°ç”¨æˆ·å¯é…ç½®çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œè®©ç”¨æˆ·å¯ä»¥è‡ªç”±ä¿®æ”¹å’Œè‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†ã€‚

---

## ğŸ¯ å®Œæˆçš„å·¥ä½œ

### 1. âœ… è¯†åˆ«éœ€è¦Promptçš„è¯„ä¼°å™¨ç±»å‹

**éœ€è¦è‡ªå®šä¹‰Prompt**ï¼š
- `Conversation Completeness` / `å¯¹è¯å®Œæ•´æ€§`
- `Role Adherence` / `è§’è‰²éµå¾ª`
- `Correctness` / `æ­£ç¡®æ€§`
- `GEval (Custom)`

**å†…ç½®æŒ‡æ ‡ï¼ˆä¸éœ€è¦Promptï¼‰**ï¼š
- `Faithfulness`
- `Answer Relevancy`
- `Contextual Precision`
- `Contextual Recall`
- `Contextual Relevancy`
- `Bias`
- `Toxicity`

### 2. âœ… æ·»åŠ è¯„ä¼°å™¨ç¼–è¾‘åŠŸèƒ½

**ä¿®æ”¹æ–‡ä»¶**: `windows/add_evaluator_window.py`

**æ–°å¢åŠŸèƒ½**ï¼š
- æ·»åŠ "è¯„ä¼°æ ‡å‡†"è¾“å…¥æ¡†ï¼ˆåŠ¨æ€é«˜åº¦Textç»„ä»¶ï¼‰
- é€‰æ‹©è¯„ä¼°å™¨ç±»å‹æ—¶è‡ªåŠ¨æ˜¾ç¤º/éšè—Promptè¾“å…¥æ¡†
- åªå¯¹éœ€è¦è‡ªå®šä¹‰çš„ç±»å‹æ˜¾ç¤ºè¾“å…¥æ¡†ï¼ˆæ–¹æ¡ˆBï¼‰
- ä¿å­˜æ—¶è‡ªåŠ¨åŒ…å«criteriaå­—æ®µ

**å…³é”®ä»£ç ** (lines 99-121):
```python
# è¯„ä¼°æ ‡å‡†ï¼ˆPromptï¼‰è¾“å…¥åŒºåŸŸ
self.criteria_frame = ttk.Frame(main_frame)

self.criteria_text = tk.Text(
    self.criteria_frame,
    font=("Arial", 10),
    height=5,  # åˆå§‹5è¡Œ
    wrap=tk.WORD,
    relief=tk.RIDGE,
    padx=5,
    pady=5
)

# åŠ¨æ€è°ƒæ•´é«˜åº¦
self.criteria_text.bind("<KeyRelease>", self._adjust_text_height)
```

### 3. âœ… åŠ¨æ€é«˜åº¦Textç»„ä»¶

**åŠŸèƒ½**ï¼š
- åˆå§‹é«˜åº¦ï¼š5è¡Œ
- å½“æ–‡æœ¬è¶…è¿‡2è¡Œåï¼šé«˜åº¦ = æ–‡æœ¬è¡Œæ•° + 3
- å®æ—¶å“åº”ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è°ƒæ•´é«˜åº¦
- æ— æ»šåŠ¨æ¡ï¼Œè§†è§‰ç¾è§‚

**å®ç°æ–¹æ³•** (lines 319-334):
```python
def _adjust_text_height(self, event=None):
    """åŠ¨æ€è°ƒæ•´Textç»„ä»¶é«˜åº¦"""
    content = self.criteria_text.get(1.0, tk.END)
    lines = content.count('\n') + 1

    # è®¡ç®—æ–°é«˜åº¦ï¼šæœ€å°‘5è¡Œï¼Œè¶…è¿‡2è¡Œå = è¡Œæ•° + 3
    if lines <= 2:
        new_height = 5
    else:
        new_height = lines + 3

    current_height = int(self.criteria_text.cget('height'))
    if new_height != current_height:
        self.criteria_text.config(height=new_height)
```

### 4. âœ… ä¿®æ”¹æ•°æ®å­˜å‚¨ç»“æ„

**ä¿®æ”¹å‰**ï¼š
```json
{
  "name": "å¯¹è¯å®Œæ•´æ€§è¯„ä¼°å™¨",
  "framework": "deepeval",
  "metric_type": "Conversation Completeness",
  "threshold": 0.6
}
```

**ä¿®æ”¹å**ï¼š
```json
{
  "name": "å¯¹è¯å®Œæ•´æ€§è¯„ä¼°å™¨",
  "framework": "deepeval",
  "metric_type": "Conversation Completeness",
  "threshold": 0.6,
  "criteria": "è¯„ä¼°å¯¹è¯å›ç­”çš„å®Œæ•´æ€§å’Œè´¨é‡ï¼š\n\n**è¯„ä¼°ç»´åº¦ï¼š**\n..."
}
```

### 5. âœ… ä¿®æ”¹Executoré€»è¾‘

**ä¿®æ”¹æ–‡ä»¶**: `evaluators/deepeval_executor.py`

**æ”¹åŠ¨** (lines 189-209):
```python
# ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„criteriaï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
criteria = self.evaluator_info.get("criteria", "")
if not criteria:
    criteria = self._get_custom_criteria(metric_type)

evaluation_params = self._get_evaluation_params(metric_type)

return GEval(
    name=metric_type,
    criteria=criteria,
    evaluation_params=evaluation_params,
    threshold=self.threshold,
    model=model,
    verbose_mode=True,
    include_reason=True
)
```

### 6. âœ… æ•°æ®è¿ç§»

**æ–°å»ºæ–‡ä»¶**: `migrate_evaluators.py`

**è¿ç§»é€»è¾‘**ï¼š
1. åŠ è½½ç°æœ‰é…ç½®
2. æ£€æŸ¥æ¯ä¸ªè¯„ä¼°å™¨æ˜¯å¦éœ€è¦criteria
3. å¦‚æœéœ€è¦ä¸”æ²¡æœ‰ï¼Œä»ä»£ç ä¸­è·å–é»˜è®¤å€¼
4. æ·»åŠ åˆ°é…ç½®æ–‡ä»¶ä¸­
5. ä¿å­˜æ›´æ–°åçš„é…ç½®

**è¿ç§»ç»“æœ**ï¼š
```
âœ… æˆåŠŸè¿ç§» 1 ä¸ªè¯„ä¼°å™¨
- è§’è‰²éµå¾ª (GEval (Custom)) â†’ å·²æ·»åŠ é€šç”¨criteria

âŠ˜ è·³è¿‡ï¼ˆå†…ç½®æŒ‡æ ‡ï¼‰ï¼š
- æ­£ç¡®æ€§ (Faithfulness) â†’ ä¸éœ€è¦criteria
- ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevancy) â†’ ä¸éœ€è¦criteria
```

---

## ğŸ“‹ ä½¿ç”¨æµç¨‹

### æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°å™¨

1. **æ‰“å¼€æ·»åŠ è¯„ä¼°å™¨çª—å£**
2. **å¡«å†™åŸºæœ¬ä¿¡æ¯**ï¼š
   - è¯„ä¼°å™¨åç§°
   - è¯„ä¼°æ¡†æ¶ï¼ˆDeepEvalï¼‰
   - é˜ˆå€¼
3. **é€‰æ‹©è¯„ä¼°å™¨ç±»å‹**ï¼š
   - é€‰æ‹©`GEval (Custom)`æˆ–å…¶ä»–è‡ªå®šä¹‰ç±»å‹
   - **è‡ªåŠ¨æ˜¾ç¤º"è¯„ä¼°æ ‡å‡†"è¾“å…¥æ¡†** âœ¨
4. **å¡«å†™è¯„ä¼°Prompt**ï¼š
   - åœ¨æ–‡æœ¬æ¡†ä¸­è¾“å…¥è¯„ä¼°æ ‡å‡†
   - æ–‡æœ¬æ¡†ä¼šæ ¹æ®å†…å®¹è‡ªåŠ¨è°ƒæ•´é«˜åº¦
5. **ä¿å­˜**

### ç¼–è¾‘è¯„ä¼°å™¨ï¼ˆåç»­åŠŸèƒ½ï¼‰

*æ³¨æ„ï¼šè¯„ä¼°å™¨ç¼–è¾‘åŠŸèƒ½å¾…å®ç°*

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### æ™ºèƒ½æ˜¾ç¤º/éšè—

**æ–¹æ³•**: `on_metric_type_change(event)` (lines 280-297)

```python
def on_metric_type_change(self, event):
    metric_type = self.metric_type_var.get()

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ˜¾ç¤ºcriteriaè¾“å…¥æ¡†
    if self._needs_criteria(metric_type):
        # æ˜¾ç¤º
        self.criteria_frame.grid(...)
        # é¢„å¡«å……é»˜è®¤prompt
    else:
        # éšè—
        self.criteria_frame.grid_forget()
```

### åˆ¤æ–­æ˜¯å¦éœ€è¦Criteria

**æ–¹æ³•**: `_needs_criteria(metric_type)` (lines 299-311)

```python
def _needs_criteria(self, metric_type: str) -> bool:
    needs_criteria_types = [
        "Conversation Completeness", "å¯¹è¯å®Œæ•´æ€§",
        "Role Adherence", "è§’è‰²éµå¾ª",
        "Correctness", "æ­£ç¡®æ€§",
        "GEval (Custom)", "Custom"
    ]
    return any(mt in metric_type for mt in needs_criteria_types)
```

### æ•°æ®ä¿å­˜

**æ–¹æ³•**: `add_evaluator()` (lines 248-254)

```python
# å¦‚æœæ˜¯è‡ªå®šä¹‰ç±»å‹ï¼Œè·å–criteria
if self._needs_criteria(metric_type):
    criteria = self.criteria_text.get(1.0, tk.END).strip()
    if not criteria:
        messagebox.showerror("é”™è¯¯", f"{metric_type} éœ€è¦å¡«å†™è¯„ä¼°æ ‡å‡†")
        return
    evaluator_config["criteria"] = criteria
```

---

## ğŸ“Š æ•ˆæœå±•ç¤º

### æ·»åŠ å†…ç½®æŒ‡æ ‡ï¼ˆå¦‚Answer Relevancyï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ·»åŠ è¯„ä¼°å™¨                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¯„ä¼°å™¨åç§°: [___]           â”‚
â”‚ è¯„ä¼°æ¡†æ¶: [DeepEval â–¼]      â”‚
â”‚ è¯„ä¼°å™¨ç±»å‹: [Answer Relevancy â–¼] â”‚
â”‚ é˜ˆå€¼: [0.6]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â† æ²¡æœ‰è¯„ä¼°æ ‡å‡†è¾“å…¥æ¡†
```

### æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆå¦‚å¯¹è¯å®Œæ•´æ€§ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ·»åŠ è¯„ä¼°å™¨                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¯„ä¼°å™¨åç§°: [___]           â”‚
â”‚ è¯„ä¼°æ¡†æ¶: [DeepEval â–¼]      â”‚
â”‚ è¯„ä¼°å™¨ç±»å‹: [Conversation Completeness â–¼] â”‚
â”‚ é˜ˆå€¼: [0.6]                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¯„ä¼°æ ‡å‡†:                    â”‚ â† è‡ªåŠ¨æ˜¾ç¤º
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ï¼ˆåŠ¨æ€é«˜åº¦æ–‡æœ¬æ¡†ï¼‰      â”‚ â”‚
â”‚ â”‚ åˆå§‹5è¡Œï¼Œè‡ªåŠ¨æ‰©å±•      â”‚ â”‚
â”‚ â”‚                        â”‚ â”‚
â”‚ â”‚                        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… ç”¨æˆ·æ— æ„ŸçŸ¥è¿ç§»

å·²è‡ªåŠ¨è¿è¡Œè¿ç§»è„šæœ¬ï¼š
- âœ… æ£€æµ‹ç°æœ‰è¯„ä¼°å™¨
- âœ… è¯†åˆ«éœ€è¦è¿ç§»çš„ç±»å‹
- âœ… è‡ªåŠ¨æ·»åŠ criteriaå­—æ®µ
- âœ… ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
- âœ… ç”¨æˆ·æ— éœ€æ‰‹åŠ¨æ“ä½œ

---

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶

1. **windows/add_evaluator_window.py**
   - æ·»åŠ criteriaè¾“å…¥æ¡†
   - å®ç°åŠ¨æ€é«˜åº¦
   - æ·»åŠ é€‰æ‹©äº‹ä»¶å¤„ç†
   - ä¿®æ”¹ä¿å­˜é€»è¾‘

2. **evaluators/deepeval_executor.py**
   - ä¼˜å…ˆè¯»å–é…ç½®ä¸­çš„criteria
   - æä¾›ä»£ç é»˜è®¤å€¼ä½œä¸ºå¤‡é€‰

3. **migrate_evaluators.py** (æ–°å»º)
   - æ•°æ®è¿ç§»è„šæœ¬
   - åŒ…å«é»˜è®¤Promptå®šä¹‰

---

## ğŸ¯ æ€»ç»“

**ä¼˜åŒ–å®Œæˆï¼**

ç°åœ¨ç”¨æˆ·å¯ä»¥ï¼š
1. âœ… æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°å™¨æ—¶ï¼Œå¡«å†™è‡ªå·±çš„è¯„ä¼°Prompt
2. âœ… Textæ¡†é«˜åº¦è‡ªåŠ¨è°ƒæ•´ï¼Œè§†è§‰ç¾è§‚
3. âœ… é…ç½®ä¿å­˜åœ¨æœ¬åœ°ï¼Œå¯ä»¥éšæ—¶ä¿®æ”¹
4. âœ… å·²æœ‰æ•°æ®è‡ªåŠ¨è¿ç§»ï¼Œæ— æ„ŸçŸ¥å‡çº§

**ä¸‹ä¸€æ­¥**: å®ç°è¯„ä¼°å™¨ç¼–è¾‘åŠŸèƒ½ï¼Œè®©ç”¨æˆ·å¯ä»¥ä¿®æ”¹å·²æœ‰çš„Prompt

---

**å®Œæˆæ—¶é—´**: 2025-01-22
**ä¼˜åŒ–çº§åˆ«**: ğŸ”´ é‡å¤§æ”¹åŠ¨ï¼ˆæ¶æ„ä¼˜åŒ–ï¼‰
**ç”¨æˆ·ä½“éªŒ**: â­â­â­â­â­
