"""
DeepEval è¯„ä¼°æ‰§è¡Œå™¨
çœŸå®è°ƒç”¨ DeepEval æ¡†æ¶è¿›è¡Œè¯„ä¼°
"""
from typing import Dict, Any

from datasets import Dataset
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from ragas import evaluate
from ragas.metrics._answer_correctness import answer_correctness
from ragas.metrics._answer_relevance import answer_relevancy
from ragas.metrics._answer_similarity import answer_similarity
from ragas.metrics._context_precision import context_precision
from ragas.metrics._context_recall import context_recall
from ragas.metrics._faithfulness import faithfulness

metric_dict={"Faithfulness":faithfulness , # å¿ å®åº¦
             "Answer Relevancy":answer_relevancy,# ç­”æ¡ˆç›¸å…³æ€§
             "Context Precision":context_precision,#ä¸Šä¸‹æ–‡ç²¾åº¦
             "Context Recall":context_recall,#ä¸Šä¸‹æ–‡å¬å›ç‡
             "Context Relevancy":context_precision, #ä¸Šä¸‹æ–‡ç›¸å…³æ€§
             "Answer Correctness":answer_correctness, #ç­”æ¡ˆæ­£ç¡®æ€§
             "Answer Similarity":answer_similarity, #ç­”æ¡ˆç›¸ä¼¼æ€§
             }

# è‡ªå®šä¹‰åµŒå…¥ç±»
class BatchingDashScopeEmbeddings(DashScopeEmbeddings):
    """è‡ªå®šä¹‰åµŒå…¥ç±»ï¼Œç¡®ä¿æ¯æ¬¡è¯·æ±‚ä¸è¶…è¿‡ DashScope API é™åˆ¶ï¼ˆ10 ä¸ªæ–‡æœ¬ï¼‰"""

    def embed_documents(self, texts):
        all_embeddings = []
        batch_size = 10  # DashScope API é™åˆ¶

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                embeddings = super().embed_documents(batch)
                all_embeddings.extend(embeddings)
            except Exception as e:
                raise RuntimeError(f"Error embedding batch {i}-{i + len(batch)}: {str(e)}") from e

        return all_embeddings

class RagasExecutor:
    """DeepEval è¯„ä¼°æ‰§è¡Œå™¨"""

    def __init__(self, evaluator_info: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            evaluator_info: è¯„ä¼°å™¨é…ç½®
                {
                    'framework': 'deepeval',
                    'metric_type': 'Faithfulness',
                    'threshold': 0.6
                }
        """
        self.evaluator_info = evaluator_info
        self.metric_type = evaluator_info['metric_type']
        self.threshold = float(evaluator_info['threshold'])

        # # æ¸…é™¤DeepEvalç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„criteria
        # self._clear_deepeval_cache()
    #
    # def _clear_deepeval_cache(self):
    #     """æ¸…é™¤DeepEvalç¼“å­˜æ–‡ä»¶"""
    #     try:
    #         import os
    #         from pathlib import Path
    #
    #         # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
    #         script_dir = Path(__file__).parent.parent
    #         cache_file = script_dir / ".deepeval" / ".deepeval-cache.json"
    #
    #         print(f"ğŸ” æŸ¥æ‰¾DeepEvalç¼“å­˜: {cache_file}")
    #         print(f"   ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {cache_file.exists()}")
    #
    #         if cache_file.exists():
    #             os.remove(cache_file)
    #             print(f"âœ… å·²æ¸…é™¤DeepEvalç¼“å­˜: {cache_file}")
    #         else:
    #             print(f"â„¹ï¸  DeepEvalç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
    #
    #     except Exception as e:
    #         print(f"âš ï¸  æ¸…é™¤DeepEvalç¼“å­˜å¤±è´¥: {e}")

    def execute(self, question: str, answer: str, context: str, model_settings: Dict) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯„ä¼°

        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: Chatbot å›ç­”
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            model_settings: å¤§æ¨¡å‹é…ç½®

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            answers = [answer]
            contexts = [[context]]
            questions=[question]
            ground_truths=[answer]
            # To dict
            data = {
                "user_input": questions,
                "response": answers,
                "retrieved_contexts": contexts,
                "reference": ground_truths
            }

            # Convert dict to dataset
            dataset = Dataset.from_dict(data)
            # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
            DASHSCOPE_API_KEY = 'sk-a9f37cda2dff4410941489bc3c53496d'
            llm = Tongyi(
                model_name="qwen-max",
                dashscope_api_key=DASHSCOPE_API_KEY
            )

            # message: <400> InternalError.Algo.InvalidParameter: Value error, batch size is invalid, it should not be larger than 10.: input.contents
            # åˆ›å»ºåµŒå…¥æ¨¡å‹
            embeddings = BatchingDashScopeEmbeddings(
                model="text-embedding-v4",
                dashscope_api_key=DASHSCOPE_API_KEY
            )
            metric=metric_dict[self.metric_type]

            # è¯„æµ‹ç»“æœ
            result = evaluate(
                dataset=dataset,
                llm=llm,
                metrics=[
                    # context_precision,  # ä¸Šä¸‹æ–‡ç²¾åº¦
                    # context_recall,  # ä¸Šä¸‹æ–‡å¬å›ç‡
                    # faithfulness,  # å¿ å®åº¦
                    # answer_relevancy,  # ç­”æ¡ˆç›¸å…³æ€§
                    metric
                ],
                embeddings=embeddings
            )
            # 6. è§£æç»“æœï¼ˆä¼ å…¥åŸå§‹æ•°æ®ï¼‰
            return {
                'success': True,
                'score': result[metric.name][0],
                'passed': result[metric.name][0]>=0.6,
                'message': '',
                'reason': '',
                'verbose_logs': '',  # æ·»åŠ è¯¦ç»†æ—¥å¿—
                'is_english': False,
                'input': {  # æ·»åŠ è¾“å…¥æ•°æ®
                    'question': question,
                    'answer': answer,
                    'context': context
                }
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }