"""
DeepEval è¯„ä¼°æ‰§è¡Œå™¨
çœŸå®è°ƒç”¨ DeepEval æ¡†æ¶è¿›è¡Œè¯„ä¼°
"""
import os
from typing import Dict, Any
from deepeval import evaluate
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    BiasMetric,
    ToxicityMetric,
    GEval
)
from deepeval.models.llms import GPTModel


class DeepEvalExecutor:
    """DeepEval è¯„ä¼°æ‰§è¡Œå™¨"""

    def __init__(self, evaluator_info: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            evaluator_info: è¯„ä¼°å™¨é…ç½®
                {
                    'framework': 'deepeval',
                    'metric_type': 'Faithfulness',
                    'threshold': 0.6
                }
        """
        self.evaluator_info = evaluator_info
        self.metric_type = evaluator_info['metric_type']
        self.threshold = float(evaluator_info['threshold'])

        # æ¸…é™¤DeepEvalç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„criteria
        self._clear_deepeval_cache()

    def _clear_deepeval_cache(self):
        """æ¸…é™¤DeepEvalç¼“å­˜æ–‡ä»¶"""
        try:
            import os
            from pathlib import Path

            # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
            script_dir = Path(__file__).parent.parent
            cache_file = script_dir / ".deepeval" / ".deepeval-cache.json"

            print(f"ğŸ” æŸ¥æ‰¾DeepEvalç¼“å­˜: {cache_file}")
            print(f"   ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {cache_file.exists()}")

            if cache_file.exists():
                os.remove(cache_file)
                print(f"âœ… å·²æ¸…é™¤DeepEvalç¼“å­˜: {cache_file}")
            else:
                print(f"â„¹ï¸  DeepEvalç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")

        except Exception as e:
            print(f"âš ï¸  æ¸…é™¤DeepEvalç¼“å­˜å¤±è´¥: {e}")

    def execute(self, question: str, answer: str, context: str, model_settings: Dict) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯„ä¼°

        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: Chatbot å›ç­”
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            model_settings: å¤§æ¨¡å‹é…ç½®

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            # 1. é…ç½® API
            os.environ["OPENAI_API_KEY"] = model_settings['api_key']
            os.environ["OPENAI_BASE_URL"] = model_settings['base_url']

            # 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
            qwen_model = GPTModel(
                model=model_settings['model_type'],
                base_url=model_settings['base_url']
            )

            # 3. åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
            test_case = self._create_test_case(question, answer, context)

            # 4. åˆ›å»ºè¯„ä¼°æŒ‡æ ‡
            metric = self._create_metric(qwen_model)

            # 5. è¿è¡Œè¯„ä¼°
            print(f"\n{'='*60}")
            print(f"å¼€å§‹è¯„ä¼°: {self.metric_type}")
            print(f"{'='*60}")

            result = evaluate(test_cases=[test_case], metrics=[metric])

            # 6. è§£æç»“æœï¼ˆä¼ å…¥åŸå§‹æ•°æ®ï¼‰
            return self._parse_result(result, question, answer, context)

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }

    def _create_test_case(self, question: str, answer: str, context: str) -> LLMTestCase:
        """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
        # å‡†å¤‡ retrieval_context
        retrieval_context = [context] if context else []

        # æ ¹æ® metric_type å†³å®šæ˜¯å¦éœ€è¦æŸäº›å‚æ•°
        if self.metric_type in ["Faithfulness", "Contextual Precision", "Contextual Recall"]:
            # è¿™äº›æŒ‡æ ‡éœ€è¦ retrieval_context
            return LLMTestCase(
                input=question,
                actual_output=answer,
                retrieval_context=retrieval_context
            )
        elif self.metric_type in ["Answer Relevancy"]:
            # Answer Relevancy éœ€è¦ retrieval_contextï¼ˆç”¨äºç”Ÿæˆåäº‹å®é—®é¢˜ï¼‰
            return LLMTestCase(
                input=question,
                actual_output=answer,
                retrieval_context=retrieval_context
            )
        elif self.metric_type in ["Bias", "Toxicity"]:
            # Bias å’Œ Toxicity åªéœ€è¦ actual_output
            return LLMTestCase(
                input=question,
                actual_output=answer
            )
        else:
            # å…¶ä»–æŒ‡æ ‡ä½¿ç”¨é€šç”¨é…ç½®
            return LLMTestCase(
                input=question,
                actual_output=answer,
                retrieval_context=retrieval_context
            )

    def _create_metric(self, model):
        """åˆ›å»ºè¯„ä¼°æŒ‡æ ‡"""
        metric_type = self.metric_type

        # Faithfulness (å¿ å®åº¦/æ­£ç¡®æ€§)
        if metric_type == "Faithfulness":
            return FaithfulnessMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,  # å¯ç”¨è¯¦ç»†æ¨¡å¼
                include_reason=True  # åŒ…å«åŸå› 
            )

        # Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§)
        elif metric_type == "Answer Relevancy" or metric_type == "ç­”æ¡ˆç›¸å…³æ€§":
            return AnswerRelevancyMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # Bias (åè§)
        elif metric_type == "Bias":
            return BiasMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # Toxicity (æ¯’æ€§)
        elif metric_type == "Toxicity":
            return ToxicityMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # Contextual Precision
        elif metric_type == "Contextual Precision":
            from deepeval.metrics import ContextualPrecisionMetric
            return ContextualPrecisionMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # Contextual Recall
        elif metric_type == "Contextual Recall":
            from deepeval.metrics import ContextualRecallMetric
            return ContextualRecallMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # Contextual Relevancy
        elif metric_type == "Contextual Relevancy":
            from deepeval.metrics import ContextualRelevancyMetric
            return ContextualRelevancyMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

        # GEval (è‡ªå®šä¹‰è¯„ä¼°)
        elif metric_type.startswith("GEval") or "Custom" in metric_type or \
             "Conversation Completeness" in metric_type or "å¯¹è¯å®Œæ•´æ€§" in metric_type or \
             "Role Adherence" in metric_type or "è§’è‰²éµå¾ª" in metric_type or \
             "Correctness" in metric_type or "æ­£ç¡®æ€§" in metric_type:
            # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„criteriaï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            criteria = self.evaluator_info.get("criteria", "")

            print("\n" + "="*60)
            print(f"åˆ›å»ºGEvalè¯„ä¼°å™¨")
            print(f"  metric_type: {metric_type}")
            print(f"  é…ç½®ä¸­çš„criteriaé•¿åº¦: {len(criteria)}")
            print(f"  é…ç½®ä¸­çš„criteriaå†…å®¹: {criteria[:100] if criteria else '(ç©º)'}...")

            if not criteria:
                print(f"  âš ï¸  é…ç½®ä¸­æ— criteriaï¼Œä½¿ç”¨é»˜è®¤å€¼")
                criteria = self._get_custom_criteria(metric_type)
            else:
                print(f"  âœ… ä½¿ç”¨é…ç½®ä¸­çš„criteria")

            evaluation_params = self._get_evaluation_params(metric_type)

            print(f"  æœ€ç»ˆä½¿ç”¨çš„criteria: {criteria[:100]}...")
            print(f"  evaluation_params: {evaluation_params}")
            print("="*60 + "\n")

            return GEval(
                name=metric_type,
                criteria=criteria,
                evaluation_params=evaluation_params,
                threshold=self.threshold,
                model=model,
                verbose_mode=True
                # æ³¨æ„ï¼šGEvalä¸æ”¯æŒinclude_reasonå‚æ•°
            )

        # é»˜è®¤ä½¿ç”¨ Faithfulness
        else:
            return FaithfulnessMetric(
                threshold=self.threshold,
                model=model,
                verbose_mode=True,
                include_reason=True
            )

    def _get_custom_criteria(self, metric_type: str) -> str:
        """è·å–è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†"""
        # æ ¹æ®ä¸åŒçš„è‡ªå®šä¹‰ç±»å‹è¿”å›ä¸åŒçš„ criteria
        if "Role Adherence" in metric_type or "è§’è‰²éµå¾ª" in metric_type:
            return """è¯„ä¼°å›ç­”æ˜¯å¦ç¬¦åˆä¸“ä¸šä¿é™©å®¢æœçš„è§’è‰²è¦æ±‚ï¼š

**è¯­æ°”è¦æ±‚ï¼š**
- ä¸“ä¸šã€ç¤¼è²Œã€å‹å¥½
- æœ‰åŒç†å¿ƒå’Œå…³æ€€
- ä½¿ç”¨é€‚å½“çš„ç§°å‘¼ï¼ˆå¦‚"æ‚¨å¥½"ã€"è¯·é—®"ï¼‰

**å†…å®¹è¦æ±‚ï¼š**
- æä¾›å‡†ç¡®çš„ä¿é™©ä¿¡æ¯
- ç»™å‡ºå…·ä½“çš„å»ºè®®
- é¿å…è¯¯å¯¼æ€§è¡¨è¿°

**ç¦å¿Œè¡Œä¸ºï¼š**
- ä¸éšæ„ã€ä¸å†·æ¼ 
- ä¸è¯´"ä½ çˆ±ä¹°ä¸ä¹°"è¿™ç§è¯
- ä¸è´¬ä½å®¢æˆ·

è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†è¯„ä¼°å›ç­”ï¼ˆ0-1åˆ†ï¼Œ1åˆ†ä¸ºå®Œå…¨ç¬¦åˆï¼‰ã€‚"""

        elif "Correctness" in metric_type or "æ­£ç¡®æ€§" in metric_type:
            return """è¯„ä¼°å›ç­”åœ¨äº‹å®ã€é€»è¾‘å’Œæ•°æ®ä¸Šçš„å‡†ç¡®æ— è¯¯ç¨‹åº¦ï¼š

**1åˆ†ï¼š** æœŸæœ›ç­”æ¡ˆä¸å®é™…ç­”æ¡ˆæè¿°çš„äº‹å®å®Œå…¨ä¸ä¸€è‡´æˆ–è¯­ä¹‰ç›¸å

**2åˆ†ï¼š** æœŸæœ›ç­”æ¡ˆä¸å®é™…ç­”æ¡ˆæè¿°çš„äº‹å®ç¨æœ‰ç›¸è¿‘ï¼Œä½†å…·ä½“ç»†èŠ‚ä¸ä¸€è‡´

**3åˆ†ï¼š** æœŸæœ›ç­”æ¡ˆä¸å®é™…ç­”æ¡ˆæè¿°çš„äº‹å®å‹‰å¼ºä¸€è‡´ï¼Œä½†ç»†èŠ‚ä¸è¶³

**4åˆ†ï¼š** æœŸæœ›ç­”æ¡ˆä¸å®é™…ç­”æ¡ˆæè¿°çš„äº‹å®ä¸€è‡´ï¼Œåªæ˜¯è¯æ±‡ç›¸è¿‘

**5åˆ†ï¼š** æœŸæœ›ç­”æ¡ˆä¸å®é™…ç­”æ¡ˆæè¿°çš„äº‹å®å®Œå…¨ä¸€è‡´ï¼Œå†…å®¹ååˆ†ä¸€è‡´

è¯·æ ¹æ®æ­¤æ ‡å‡†è¯„åˆ†ï¼ˆ0-1åˆ†ï¼‰ã€‚"""

        elif "Conversation Completeness" in metric_type or "å¯¹è¯å®Œæ•´æ€§" in metric_type:
            return """è¯„ä¼°å¯¹è¯å›ç­”çš„å®Œæ•´æ€§å’Œè´¨é‡ï¼š

**è¯„ä¼°ç»´åº¦ï¼š**

1. **ä¿¡æ¯å®Œæ•´æ€§ï¼ˆ40%ï¼‰ï¼š**
   - æ˜¯å¦å›ç­”äº†ç”¨æˆ·é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
   - æ˜¯å¦æä¾›äº†å¿…è¦çš„å…³é”®ä¿¡æ¯
   - æ˜¯å¦é—æ¼äº†é‡è¦ç»†èŠ‚

2. **é€»è¾‘è¿è´¯æ€§ï¼ˆ30%ï¼‰ï¼š**
   - å›ç­”æ˜¯å¦å‰åä¸€è‡´ã€é€»è¾‘æ¸…æ™°
   - è®ºè¿°æ˜¯å¦æœ‰æ¡ç†ã€å±‚æ¬¡åˆ†æ˜
   - æ˜¯å¦å­˜åœ¨è‡ªç›¸çŸ›ç›¾çš„å†…å®¹

3. **å†…å®¹å……åˆ†æ€§ï¼ˆ30%ï¼‰ï¼š**
   - è§£é‡Šæ˜¯å¦å……åˆ†ã€æ˜“äºç†è§£
   - æ˜¯å¦æä¾›äº†å…·ä½“çš„ä¾‹å­æˆ–è¯´æ˜
   - æ˜¯å¦é¿å…äº†è¿‡äºç®€ç•¥æˆ–å«ç³Š

**è¯„åˆ†æ ‡å‡†ï¼š**

**1åˆ†ï¼ˆ0.0-0.2ï¼‰ï¼šä¸å®Œæ•´**
- å›ç­”ä¸¥é‡ä¸å®Œæ•´ï¼Œåªå›ç­”äº†é—®é¢˜çš„å¾ˆå°ä¸€éƒ¨åˆ†
- ç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Œç”¨æˆ·éœ€è¦å¤šæ¬¡è¿½é—®
- é€»è¾‘æ··ä¹±ï¼Œå‰åçŸ›ç›¾

**2åˆ†ï¼ˆ0.2-0.4ï¼‰ï¼šè¾ƒä¸å®Œæ•´**
- å›ç­”äº†éƒ¨åˆ†é—®é¢˜ï¼Œä½†é—æ¼é‡è¦ä¿¡æ¯
- é€»è¾‘åŸºæœ¬æ¸…æ¥šï¼Œä½†æœ‰äº›åœ°æ–¹ä¸å¤Ÿè¿è´¯
- å†…å®¹ä¸å¤Ÿå……åˆ†ï¼Œéœ€è¦è¡¥å……è¯´æ˜

**3åˆ†ï¼ˆ0.4-0.6ï¼‰ï¼šä¸­ç­‰å®Œæ•´**
- å›ç­”äº†é—®é¢˜çš„ä¸»è¦æ–¹é¢ï¼Œä½†ç»†èŠ‚ä¸å¤Ÿ
- é€»è¾‘åŸºæœ¬æ¸…æ™°ï¼Œå¶æœ‰ä¸è¿è´¯ä¹‹å¤„
- å†…å®¹åŸºæœ¬å……åˆ†ï¼Œä½†å¯ä»¥æ›´è¯¦ç»†

**4åˆ†ï¼ˆ0.6-0.8ï¼‰ï¼šè¾ƒå®Œæ•´**
- å›ç­”äº†é—®é¢˜çš„å¤§éƒ¨åˆ†æ–¹é¢ï¼Œç»†èŠ‚è¾ƒå®Œæ•´
- é€»è¾‘æ¸…æ™°è¿è´¯ï¼Œè®ºè¿°æœ‰æ¡ç†
- å†…å®¹å……åˆ†ï¼Œæä¾›äº†é€‚å½“çš„è§£é‡Š

**5åˆ†ï¼ˆ0.8-1.0ï¼‰ï¼šå®Œæ•´**
- å…¨é¢å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢ï¼Œä¿¡æ¯å®Œæ•´
- é€»è¾‘æ¸…æ™°ä¸¥å¯†ï¼Œè®ºè¿°å±‚æ¬¡åˆ†æ˜
- å†…å®¹éå¸¸å……åˆ†ï¼Œè§£é‡Šè¯¦ç»†æ˜“æ‡‚

è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†è¯„ä¼°å¯¹è¯å®Œæ•´æ€§ï¼ˆ0-1åˆ†ï¼‰ã€‚"""

        else:
            return """è¯·è¯„ä¼°å›ç­”çš„è´¨é‡ï¼Œè€ƒè™‘å‡†ç¡®æ€§ã€ç›¸å…³æ€§å’Œå®Œæ•´æ€§ã€‚"""

    def _get_evaluation_params(self, metric_type: str) -> list:
        """è·å– evaluation_params"""
        # æ ¹æ®ä¸åŒçš„è‡ªå®šä¹‰ç±»å‹è¿”å›ä¸åŒçš„å‚æ•°
        if "Role Adherence" in metric_type or "è§’è‰²éµå¾ª" in metric_type:
            return [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
        elif "Correctness" in metric_type or "æ­£ç¡®æ€§" in metric_type:
            return [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        elif "Conversation Completeness" in metric_type or "å¯¹è¯å®Œæ•´æ€§" in metric_type:
            return [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
        else:
            return [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]

    def _parse_result(self, result, question: str, answer: str, context: str) -> Dict[str, Any]:
        """è§£æè¯„ä¼°ç»“æœ"""
        try:
            # è·å–æµ‹è¯•ç»“æœ
            test_result = result.test_results[0]

            # è·å–æŒ‡æ ‡æ•°æ®ï¼ˆæ³¨æ„ï¼šæ˜¯ metrics_data ä¸æ˜¯ metrics_resultsï¼‰
            metrics_data = test_result.metrics_data

            if not metrics_data or len(metrics_data) == 0:
                return {
                    'success': False,
                    'error': 'No metrics data returned',
                    'message': 'è¯„ä¼°å¤±è´¥: æœªè¿”å›æŒ‡æ ‡æ•°æ®'
                }

            metric_data = metrics_data[0]

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if hasattr(metric_data, 'error') and metric_data.error:
                return {
                    'success': False,
                    'error': metric_data.error,
                    'message': f"è¯„ä¼°å¤±è´¥: {metric_data.error}"
                }

            # æå–åˆ†æ•°
            score = metric_data.score if metric_data.score is not None else 0.0
            passed = metric_data.success

            # è·å–åŸå› ï¼ˆå¦‚æœæœ‰ï¼‰
            reason = metric_data.reason if metric_data.reason else ""
            if not reason:
                reason = "è¯„ä¼°å®Œæˆ" if passed else "æœªè¾¾åˆ°é˜ˆå€¼"

            # è·å–è¯¦ç»†æ—¥å¿—ï¼ˆå¦‚æœæœ‰ï¼‰
            verbose_logs = getattr(metric_data, 'verbose_logs', None)

            # è°ƒè¯•ï¼šæ‰“å°verbose_logsä¿¡æ¯
            print(f"\n{'='*60}")
            print(f"Debug - verbose_logs ä¿¡æ¯:")
            print(f"  æ˜¯å¦å­˜åœ¨: {verbose_logs is not None}")
            print(f"  ç±»å‹: {type(verbose_logs)}")
            print(f"  é•¿åº¦: {len(verbose_logs) if verbose_logs else 0}")
            if verbose_logs and len(verbose_logs) > 0:
                print(f"  å‰500å­—ç¬¦: {verbose_logs[:500]}")
            print(f"{'='*60}\n")

            # æ„å»ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«è¾“å…¥æ•°æ®ï¼‰
            detail_info = {
                'success': True,
                'score': score,
                'passed': passed,
                'reason': reason,
                'verbose_logs': verbose_logs,
                'input': {  # æ–°å¢ï¼šåŒ…å«è¾“å…¥æ•°æ®
                    'question': question,
                    'answer': answer,
                    'context': context
                }
            }

            # æ„å»ºæ¶ˆæ¯
            message = f"è¯„ä¼°å™¨ï¼š{self.evaluator_info.get('name', '')}\n"
            message += f"æ¡†æ¶ï¼šDeepEval\n"
            message += f"ç±»å‹ï¼š{self.metric_type}\n\n"
            message += f"{'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}\n"
            message += f"å¾—åˆ†ï¼š{score:.3f} / {self.threshold}\n\n"
            message += f"è¯„ä¼°åŸå› ï¼š\n{reason}\n"

            # æ£€æµ‹æ˜¯å¦ä¸ºè‹±æ–‡
            is_english = self._is_english_text(reason)

            return {
                'success': True,
                'score': score,
                'passed': passed,
                'message': message,
                'reason': reason,
                'verbose_logs': verbose_logs,  # æ·»åŠ è¯¦ç»†æ—¥å¿—
                'is_english': is_english,
                'input': {  # æ·»åŠ è¾“å…¥æ•°æ®
                    'question': question,
                    'answer': answer,
                    'context': context
                }
            }

        except Exception as e:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            import traceback
            error_details = f"{str(e)}\n\n"
            error_details += f"Result type: {type(result)}\n"
            error_details += f"Result attributes: {([a for a in dir(result) if not a.startswith('_')])}\n"

            if hasattr(result, 'test_results'):
                error_details += f"\ntest_results length: {len(result.test_results)}\n"
                if len(result.test_results) > 0:
                    test_result = result.test_results[0]
                    error_details += f"test_result type: {type(test_result)}\n"
                    error_details += f"test_result attributes: {([a for a in dir(test_result) if not a.startswith('_')])}\n"

            error_details += f"\n{traceback.format_exc()}"

            return {
                'success': False,
                'error': str(e),
                'message': f"ç»“æœè§£æå¤±è´¥: {str(e)}",
                'debug_info': error_details
            }

    def _is_english_text(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡"""
        if not text:
            return False

        # ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœä¸­æ–‡å­—ç¬¦å°‘äº 20%ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        total_chars = len(text)

        if total_chars == 0:
            return False

        chinese_ratio = chinese_chars / total_chars
        return chinese_ratio < 0.2
